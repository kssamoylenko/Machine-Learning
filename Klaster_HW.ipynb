{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pm.MorphAnalyzer()#лемматизируем с помощью pymorphy. работает долго)\n",
    "def words(str):\n",
    "    nolinks = ' '.join([word for word in str.split() if (not re.findall('https?://\w.*|\w+\.\w+', word))])#удалили ссылки\n",
    "    clean_line = re.sub('[\\W\\d_-]+', ' ', nolinks.lower().strip())\n",
    "    ws = re.split(' +', clean_line)\n",
    "    return [morph.parse(w)[0].normal_form for w in ws]\n",
    "df = pd.read_csv('raw_news.csv', index_col=0)\n",
    "no_dup_df = df.drop_duplicates(keep='first')\n",
    "texts = no_dup_df['text'] #прочитали тексты\n",
    "text_list = [' '.join(words(text)) for text in texts] # список списков слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import *\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('vect', CountVectorizer(max_df=0.9, min_df=0.20, ngram_range=(1,3), analyzer='word')),\n",
    "      ('tfidf', TfidfTransformer()),\n",
    "      ('svd', TruncatedSVD(n_components=133)),\n",
    "      ('norm', Normalizer() ),\n",
    "      ('clust', KMeans(n_clusters=28))\n",
    "])\n",
    "\n",
    "pipeline.fit(text_list)\n",
    "#применили все методы, которые были в лабораторной.как сингулярное разложение и tfidf влияют на качество кластеризации надо\n",
    "#посмотреть на нескольких примерах и придумать, как их вывести (сингулярное вроде не особо, tfidf очень сильно)\n",
    "explained_variance = pipeline.named_steps['svd'].explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "      ('vect', CountVectorizer(max_df=0.9, min_df=0.10, ngram_range=(1,5), analyzer='word')),\n",
    "      ('tfidf', TfidfTransformer()),\n",
    "      ('svd', TruncatedSVD(n_components=50)),\n",
    "      ('norm', Normalizer() ),\n",
    "      ('clust', KMeans(n_clusters=28))\n",
    "])\n",
    "\n",
    "pipeline.fit(text_list)\n",
    "\n",
    "explained_variance = pipeline.named_steps['svd'].explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df['event_id']\n",
    "print(len(set(clust_labels)))\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Задание 3 (задали маленькое число кластеров)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list2 = text_list\n",
    "pipeline = Pipeline([\n",
    "      ('vect', CountVectorizer(max_df=0.9, min_df=0.1, ngram_range=(1,5), analyzer='word')),\n",
    "      ('tfidf', TfidfTransformer()),\n",
    "      ('svd', TruncatedSVD(n_components=50)),\n",
    "      ('norm', Normalizer() ),\n",
    "      ('clust', KMeans(n_clusters=5))\n",
    "])\n",
    "\n",
    "pipeline.fit(text_list2)\n",
    "\n",
    "explained_variance = pipeline.named_steps['svd'].explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df['event_id']\n",
    "print(len(set(clust_labels)))\n",
    "#как видно из результатов, полнота высокая, а вот Homogeneity сильно упала по сравнению с теми же параметрами, но для 28 кластеров\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#не очень симпатинчй\n",
    "class TextProcessor():\n",
    "    def __init__(self):\n",
    "        self.ws = {}\n",
    "    def send(self, word):\n",
    "        if(not (word in self.ws)):\n",
    "            self.ws[word] = 0\n",
    "        self.ws[word] += 1\n",
    "    def words(self):\n",
    "        return self.ws\n",
    "    def sorted(self, count):\n",
    "        all = sorted(list(self.ws.items()), key=lambda w: -w[1])\n",
    "        return all[:count]\n",
    "realKeys = {}\n",
    "for i in range(len(clust_labels)):\n",
    "    if (not clust_labels[i] in realKeys):\n",
    "        realKeys[clust_labels[i]] = []\n",
    "    realKeys[clust_labels[i]].append(labels[i])\n",
    "realSorted = {}\n",
    "for k, v in realKeys.items():\n",
    "    proc = TextProcessor()\n",
    "    for w in v:\n",
    "        proc.send(w)\n",
    "    realSorted[k] = proc.sorted(100)\n",
    "print(realSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = pd.read_csv('events.csv', index_col=0)\n",
    "evs = {}\n",
    "for k in range(len(ef['name'])):\n",
    "     evs[k + 1] = ef['name'][k+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty = {}\n",
    "for k, v in realSorted.items():\n",
    "    pretty[k] = [(evs[e[0]], e[1]) for e in v]\n",
    "print(pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
